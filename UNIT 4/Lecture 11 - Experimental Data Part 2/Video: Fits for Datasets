Why We Build Models
-help us understand the process that generated teh data
-help us make predictions about out of sample data
-a good model helps us do these things

Degree 16 is Tightest Fit
-training error
-how well model performs on data which it was learned
-small training necessary condition for great model, but not sufficient one
-we want model to work well on other data generated by same process
-it needs to generalize

Cross Validate
-generate using 1 dataset, test it on the other
-use models for 1 to predict points for 2 and vice versa
-expect testing error to be larger than training error
-better indication of generalizibility

def genNoisyParabolicData(a, b, c, xVals, fName):
    yVals = []
    for x in xVals:
        theoreticalVal = a*x**2 + b*x + c
        yVals.append(theoreticalVal\
        + random.gauss(0, 35))
    f = open(fName,'w')
    f.write('x        y\n')
    for i in range(len(yVals)):
        f.write(str(yVals[i]) + ' ' + str(xVals[i]) + '\n')
    f.close()
    
#parameters for generating data
xVals = range(-10, 11, 1)
a, b, c = 3, 0, 0
degrees = (2, 4, 8, 16)

#generate data
random.seed(0)
genNoisyParabolicData(a, b, c, xVals,
                      'Dataset 1.txt')

genNoisyParabolicData(a, b, c, xVals,
                      'Dataset 2.txt')

xVals1, yVals1 = getData('Dataset 1.txt')
models1 = genFits(xVals1, yVals1, degrees)
testFits(models1, degrees, xVals1, yVals1,
        'DataSet 1.txt')

pylab.figure()
xVals2, yVals2 = getData('Dataset 2.txt')
models2 = genFits(xVals2, yVals2, degrees)
testFits(models2, degrees, xVals2, yVals2,
         'DataSet 2.txt')
         
#cross validation
pylab.figure()
testFits(models1, degrees, xVals2, yVals2,
         'DataSet 2/Model 1')
pylab.figure()
testFits(models2, degrees, xVals1, yVals1,
         'DataSet 1/Model 2')
